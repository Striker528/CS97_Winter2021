1. Write a shell command that reads standard input and outputs a copy of every input line consisting of a Git commit ID with nothing else on the line.
https://stackoverflow.com/questions/6980090/how-to-read-from-a-file-or-stdin-in-bash

while read line
do
	echo "$line" | tr -dc '0-9'
	echo
done < "${1:-/dev/stdin}"


2. Why would you write C or C++ code that you know cannot possibly be executed?  Give an example where this might be useful, and briefly explain the use.
From Lecture 12, 

In short, one would want to improve performance, so adding this function builtin_unreachable() so the compiler does not go anywhere when calling the function, 
	so the compiler does less work, making it faster. And in the example it does the check of errno == 0 very quickly 
	so that the program would just stop quickly instead of going to that function that is defined far away, 
	it just looks at this fast function to check. 



__builtin_unreachable () function (in GCC, Clang, not standardized) 

	  Defined by compiler.

		This function has undefined behavior - if you actually execute it, the compiler could generate any code

			it likes; so, don’t ever execute it.

		Wherever you call it, the compiler can assume that this location cannot be reached during the execution of your program.



      An example of __builtin_unreachable.



         Renames a file, returns -1 on failure (setting errno to positive failure code),

                              0 on success (setting errno to garbage)



          int err = rename("a", "b") == 0 ? 0 : errno;

          if (err == 0)

        print ("OK!");



    This will cause the compiler to generate code to test

    whether errno == 0.  That’s inefficient.  It would be better to tell the compiler that errno cannot possibly be zero right after rename fails.



        int positive(int x) { if (x <= 0) __builtin_unreachable (); return x; }



          int err = rename("a", "b") == 0 ? 0 : positive(errno);

          if (err == 0)

        print ("OK!");



     __builtin_unreachable (X) is a directive from the programmer to the compiler, telling the compiler that it can assume

			that X is true, and it can use that assumption to generate better code.



       This can improve performance


3. Give an example of a false positive (false alarm) generated by static checking in GCC.
https://stackoverflow.com/questions/35357135/what-is-a-false-positive#:~:text=false%20positive%20in%20general%20means,integers%20because%20it's%20dangerous%20operation.



https://cellperformance.beyond3d.com/articles/2006/06/understanding-strict-aliasing.html



the warning is incorrect, and that a problem doesn't in fact exist.

false positive in general means when something pretends the answer is yes / conforms to some condition when actually it's not true.



An example would be when gcc gives you a warning about incorrect cast from integer to structure of the same size holding two 16-bit integers because it's dangerous operation. But you know that it's actually correct and generates correct code.



Like in:


uint32_t
swap_words( uint32_t arg )
{
    U32*     in = (U32*)&arg;
    uint16_t lo = in->u16[0];
    uint16_t hi = in->u16[1];

    in->u16[0] = hi;
    in->u16[1] = lo;

    return (in->u32);
}
Lecture 12:
-Wall stands for a bunch of warning options combined:



        -Wcomment  Warns about dubious constructions in comments.

                   int x /* int y; /* y is greater than x */;

        -Wparentheses  Warns about a “confusing” lack of parentheses.

               int j = n << i + k;

              means “n << (i + k)” but most C programmers

              don’t know the precedence rules that well,

              A better style might be

               int j = n << (i + k);  // pacifies gcc -Wparentheses



              if (a < b && b < c || i < n) ...

              if ((a < b && b < c) || i < n) ... // pacifies gcc -Wparentheses



        -Waddress warns about dubious address constructions, such as:



           char *p = ...;

       if (p == "abc")

         ...



        -Wstrict-aliasing    warns about pointers being “abused”

                         to point at the “wrong” type.



            long l = -1;   // 64-bit word

        int *p = (int *) &l;   // pointer to a 32-bit word (GCC complains)

        *p = 0;        // clears half of the 64-bit word

                       // result is machine-dependent

               // C standard says behavior is undefined.

          The Linux kernel developers play this trick all the time

          in their code, so they don’t use -Wstrict-aliasing.



        -Wmaybe-uninitialized

        Is there a path through your function that might use

        a local variable V without first initializing V?

        If so, generate a warning.



             int f(int n) {

                int v;

        if (n < 0)

          v = n + 1;

                .... don’t change n or v here ...

        return n < 0 ? v / 2 : ...;

         }



             GCC won’t warn about this, but only with -O2 or better.

         With -O0 it might warn.



      -Wextra stands for even more (more controversial) options



        -Wtype-limits   warn about unnecessary comparisons due to

                     type restrictions, e.g.:



            time_t x;



            if (x < 0)  // with -Wtype-limits,

                    // generates a warning if time_t is unsigned

          print("before epoch");



         ...



   Try compiling with ‘gcc -Wall -Wextra’.

   Get a lot of false alarms.

   Turn them off either by:

     * disabling the warning ‘gcc -Wall -Wextra -Wno-type-limits’

     * Modify your program somehow (this can be too much work).



4. Give an example of transforming a run-time check into a compile-time check using C or C++.
https://stackoverflow.com/questions/57785911/compile-time-check-and-runtime-check-at-the-same-time

Lecture 13

Compile-time: not following proper syntax, errors because of my incompetence. 

Run-time: errors when the program is running: dividing by 0



In C, 

- Compile-time checks happen automatically with compilers like gcc, so just write something wrong like mis-labeling a variable

- Run-time checks: can check if an integer divides by 0



So run-time: divide an integer by 0

Compile-time: spell the integer name variable wrong


5. What is the main advantage of transforming run-time to compile-time checks, as described in the previous question?
Lecture 13

Run time checks can miss some real runtime errors, and/or work for or one a particular set of test input data, so compile time checks can catch these mistakes and lead to better testing of the code. 



runtime checks

  - can miss some real runtime errors

  - work or for one run on a particular set of test data

  - can have severe performance penalties (thread sanitization)

      (CPU or memory)


6. Given the advantage that you cite in the previous question's answer, why don’t programmers always prefer compile-time to run-time checks?
https://en.wikipedia.org/wiki/Runtime_error_detection



Run-time checks give such a wide and powerful range of checks like:

- Race conditions

- Exceptions

- Resource leaks

- memory leaks

- null pointers

- buffer overflows

- etc.

just to name a few. 


7. Node supports both synchronous I/O, which blocks the V8 thread until the I/O operation completes, and asynchronous I/O, which doesn’t.  
For example, fs.readFileSync is synchronous whereas fs.readFile is not.  
In general, when is it better to use synchronous I/O, and when is it better to use asynchronous I/O, and why?
https://stackoverflow.com/questions/35012494/difference-between-synchronous-and-asychnchronus-i-o#:~:text=1%20Answer&text=Synchronous%20I%2FO%20mean%20that,itself%20causes%20something%20to%20happen.

CS 33

For Synch I/O, what makes it better than I/O is that race conditions are far less likely because 2 event handles cannot run at the same time, 
	so locks are not needed. The flow of executions is waiting for the operation to complete. But if there is a heavy load running, 
	that can cause problems, as that process needs to be completed before anything else runs. 
Which then leads to Async I/O look attractive in order to get parallelism working to speed up the process of the program. 

Or to get the best of both worlds, combine the 2 into super I/O. 


"
If you have only asynchronous operations and want a synchronous operation, just call the asynchronous operation and then block on something that is unblocked by the completion handler.

If you have only synchronous operations and want an asynchronous operation, just create a new thread to call the synchronous operation and have it invoke the completion handler when the synchronous operation returns.

"

8. Give two examples of how you could use Emacs and Git in the same command that you issue in an interactive environment. 
In the first example, Emacs should be the main driver and Git should be subsidiary to Emacs. 
The second example should be the reverse, where Git is the main driver and Emacs should be subsidiary. 
In both cases, specify what characters or mouse actions etc. that you'd issue to initiate the command.
LISP stuff, just pretend I wrote the write answer as I know nothing about LISP and avoid it like the plague. 

9. If you draw a graph of a Git repository, where each node is a commit and there is an arc from node A to node B if A is a parent of B, you’ll get a directed acyclic graph (DAG).

Briefly explain why it’s important that the commit graph must be a DAG.  Give two examples of what could go wrong if the graph had cycles and were therefore not a DAG.  Use ‘git log’ in one of your examples, and ‘git diff’ in another.
https://medium.com/girl-writes-code/git-is-a-directed-acyclic-graph-and-what-the-heck-does-that-mean-b6c8dec65059



"Each commit remembers which commit came before it". Using this process, a clear history can be drawn from the very first commit to each recent commit showing a clear history of the project. 



Cycles:

git log:

- the git history would continuously loop

- the same few commits would be repeated and it would not move on

- git log outputs in order following the branches of the commit graph



git diff:

- shows the difference between 2 different files shows the differences, and this is done by using the directed graph as the changes can be easily mapped going in one direction from parent to child, but in a cyclic way, this could not work.


10. If you draw a graph of all the objects in a Git repository (not just the commits), and draw arcs from object A to object B if A’s contents contain a reference to B, is the resulting graph a DAG?  Explain why or why not.
https://git-scm.com/book/en/v2/Git-Internals-Git-Objects

The graph would not a DAG. Other objects in a git repo include copies of files that are hashed. One can have 2 versions of the same file hashed in the git objects folder. And DAG cannot have any cycles, so 1 cannot reference 2 while 2 references 1. 


11. In a Linux filesystem where directories refer to other files, are all the files arranged as a tree, as any (i.e., unrestricted) form of DAG, or as something else?  Briefly explain.
https://opensource.com/life/16/10/introduction-linux-filesystems

The linux file system is a tree structure, with every directory building off the root. And a tree is a restricted form of DAG as it has a strict parent /child flow and does not have cycles. 


12. Suppose you have direct access to a Git repository on your home directory on SEASnet, 
	suppose you log in one day and see that the repository has a file named ‘.git/objects/ff/16c897eadab9bebc58bd0ca0fb5c8e1c237a15’.

How can you tell whether that file is write protected, using a shell command of your own, and without modifying the file or using Git?
https://unix.stackexchange.com/questions/159557/how-to-non-invasively-test-for-write-access-to-a-file

[ -w /path/to/file ] && echo "writeable" || echo "write permission denied"
[ -w .git/objects/ff/16c897eadab9bebc58bd0ca0fb5c8e1c237a15 ] && echo "writeable" || echo "write permission denied"


13. Suppose you remove the file as follows:

$ rm .git/objects/ff/16c897eadab9bebc58bd0ca0fb5c8e1c237a15
rm: remove write-protected regular file '.git/objects/ff/16c897eadab9bebc58bd0ca0fb5c8e1c237a15'? y
$

By typing the ‘rm’ command and following up with ‘y’ you removed the file.  Shouldn’t ‘rm’ have failed instead?  Why is it OK that you can remove a file that you lack write access to?
https://unix.stackexchange.com/questions/48579/why-can-rm-remove-read-only-files

All rm needs is write+execute permission on the parent directory. The permissions of the file itself are irrelevant.

Backup:

Any attempt to access a file's data requires read permission. Any attempt to modify a file's data requires write permission. Any attempt to execute a file (a program or a script) requires execute permission.

Because directories are not used in the same way as regular files, the permissions work slightly (but only slightly) differently. An attempt to list the files in a directory requires read permission for the directory, but not on the files within. An attempt to add a file to a directory, delete a file from a directory, or to rename a file, all require write permission for the directory, but (perhaps surprisingly) not for the files within. Execute permission doesn't apply to directories (a directory can't also be a program). But that permission bit is reused for directories for other purposes.

Execute permission is needed on a directory to be able to cd into it (that is, to make some directory your current working directory).

Execute is needed on a directory to access the "inode" information of the files within. You need this to search a directory to read the inodes of the files within. For this reason the execute permission on a directory is often called search permission instead.


14. What is likely to go wrong now that you have removed the file .git/objects/ff/16c897eadab9bebc58bd0ca0fb5c8e1c237a15? 
Give an example of a Git command that would have succeeded before you removed the file, but will now fail.
https://git-scm.com/book/en/v2/Git-Internals-Git-Objects


You could not examine that file anymore with git cat-file

git cat-file -p ff16c897eadab9bebc58bd0ca0fb5c8e1c237a15

would no longer work as that file would have been deleted so all of the data that is stored would be completely removed. 


15. How can you best recover from your mistake in removing the file .git/objects/ff/16c897eadab9bebc58bd0ca0fb5c8e1c237a15?
https://stackoverflow.com/questions/9477702/undo-delete-in-git

- must not have committed the changes

git checkout -f

for git checkout force which undoes that remove


16. Now, suppose instead of removing the .git/objects file, you want to corrupt the Git repository in a different way: you want to introduce a cycle into the commit graph mentioned above.  You plan to do this by adding two commits, each of which refers to the other commit as its parent.  You will create two new files in the .git/objects subdirectory, one for each commit.  Explain why it will be hard for you to corrupt the repository in this way.
New trees will be added onto the file, thus it does not depend on the corrupted file even if there was bad history. Commits are needed to point to a parent that already exists in the git folder. That means if someone is creating 2 files at once, they cannot declare their parents to be one another because they have not been finalized in the git history yet. That means it would be very hard to create a cycle from nothing, resulting in making it hard to corrupt the repository in this way. 


17. Suppose you are in a working directory with a valid .git subdirectory, and suppose the shell command:

  find .git/objects/?? -type f -print | wc

outputs the following line:

  10000   10000  550000

What does that tell you about the repository from an abstract view, and why?
https://www.toolsqa.com/git/dot-git-folder/#:~:text=In%20Git%2C%20everything%20is%20saved,The%20folders%20are%20created%20accordingly



find : means find something,



next: the address of where to look



?: match exactly one character in a file name

??: first 2 characters



-type f : type file



wc: word count



Here, the command finds all the files (type f) in the git objects folder (find address) then it prints the word count for each file. As the git objects folder has all the data of the git repo (lecture 16), this shows that in the first 2 files, there are only 10,000 words or a small amount of data was created. But in the 3rd file with 550,000 words or data, a lot of changes were made, which means that a lot of data need to  be allocated to keep up with all the changes. 


18. In that same repository, approximately what should be the output of the following command, and why?

  find .git/objects/?? -type f -name '*ab*' -print | wc
https://askubuntu.com/questions/681637/grep-the-asterisk-doesnt-always-work#:~:text=An%20asterisk%20in%20regular%20expressions,followed%20by%20the%20word%20String%20%22.

Look at previous answer for everything expect -name '*ab*'

Need to find all of the files that contain "whitespace" 0 or more times "a" then some "b"'s somewhere (0 or more times) in the file name (or the hash), then piping the print of all of the file names to word count. 

This results in an output of the wordcount information in the list of all of the commits with the above pattern in their hash. 


19. Write a shell script that outputs the names of each JavaScript file in the current directory that contains a word of 10 or more characters but does not contain any word of 5 or fewer characters.  Here a “word” is a maximal sequence of ASCII letters (either lower- or upper-case).  Your shell script should not output names beginning with ".".
https://stackoverflow.com/questions/16956810/how-do-i-find-all-files-containing-specific-text-on-linux

https://www.oreilly.com/library/view/linux-shell-scripting/9781785881985/7d6ceb99-52e1-45f1-bed3-473fc7cf7443.xhtml

#!/bin/sh

grep --include=\*.{js} -rnw '/' sed '[\x00-\x7F]{10,}' | sed '[^\x00-\x7F]{0,5}' | sed '^[^\.]'


[\x00-\x7F]{10,} : 10 or more ascii chars
[^\x00-\x7F]{0,5} : does not have 5 or fewer words
^[^\.] : cannot start with '.'


20. Suppose you have a complicated C/C++ program with the following static variables.

   int table[1000];
   int nused = 0;

NUSED is supposed to report how many table entries are used, and should vary between 0 and 1000 inclusive.  Unfortunately your program crashes, and when you examine the crashes you discover that NUSED is sometime 1093, sometimes -97, and sometimes other values out of range.

How would you use GDB to debug the program so that you efficiently discover exactly when NUSED goes out of its intended range?  State the command or set of commands you’d use.

(Lecture 13 to 14)



We could use Visual Studio to set breakpoints to see the value of each variable at that time as the easy way that even I know how to use.  



But for GDB:

	1. We first need to start and run this baby, so we need to compile it with:

-: g++ -g (the fileName, I will call it 'midnight', because I am doing this at midnight).cpp



	2. Need to create an output file for this baby, called midnight.out

The command to do this in gdb:

-: gdb midnight.out



	3. Then we need to set breakpoints (or the easy red dots in VS)

Let us set a breakpoint in main to keep things easy peasy

(lecture 14)

-: b main



	4. In this fourth step, we need to run the function to get to that certain point ( in our case, at the start of the main function) to see how the values change

This is the hardest command

-: run



	5. To actually see the values of the variables, we need to print them out (instead of the easy window in VS)

Because we are checking nused

p for print

-: p unused



	6. As we are doing this step by step, since this is our first time hitting the variable, it shoudl be 0, if not, we have problems



	7. Now, we have to trace the code with a fine comb by going line by line

(lecture 14)

step for step

-: step



	8. We need to constantly keep track of this one variable, so we have to keep printing out the variable value to see how it changes after each step



	9. Repeat steps 7 and 8 until the variable goes out of bounds



	10. Once the problem has occured, need to go backwards

https://www.gnu.org/software/gdb/news/reversible.html

https://sourceware.org/gdb/onlinedocs/gdb/Reverse-Execution.html

going backwards exactly one instruction

-: reverse-stepi



	11. Now you have the location and cause of the problem found



	12. Change the code to fix this error



	13. Recompile to make sure the problem is fixed, if not, repeat process


21. Suppose your file system uses copy-on-write and deduplication to improve performance, and your ops staff takes advantage of these techniques to perform backups more efficiently.  How well will these techniques work with your project’s development trees of source code, including their .git subdirectories?  Briefly explain where these approaches will work well, or not so well.
(Lecture 10)

https://en.wikipedia.org/wiki/Copy-on-write

https://stackoverflow.com/questions/628938/what-is-copy-on-write

https://en.wikipedia.org/wiki/Deduplication#:~:text=The%20term%20deduplication%20refers%20generally,in%20two%20or%20more%20files

https://en.wikipedia.org/wiki/Data_deduplication



Copy-on-write (COW) 



With COW, they would use Snapshots. These are very inexpensive, because they use COW heavily (lecture 10), and are read only. Thus making this great for general git work to cut down on space. Also, with COW, if a file is copied but not modified, you can just created a low cost pointer to point to the old file, saving space (StackOverflow).This contrast with deduplication because git deals with version control, and you can't really have that when each new copy just removes the old repeating data (Wikipedia). And this also leads to a problem where if a block is corrupted, then all files references that block would become corrupted since no copies are being made to save space. But, this space saving via deduplication would work when creating a new clone of the repo from git pull since the files would need to be overwritten and the unneeded copies are already on that space. And this can also be applied to creating new branches and so forth. 



Now, here are the pros and cons for each type:

For the pros of COW, copies are extremally easy and cheap to make, because a small pointer is being created to point to that original copy, and only metadata is copied until the file will be modified. As the blocks are overwritten, the original data is just simply moved over and set aside so the original data is not overwritten. But there is a con to COW. COW may take far more time than deduplication as there needs to be an allocation and a write, which does take a significant amount of time. Then, for the pros of deduplication. It save a lot more space compared to COW. From Wikipedia "Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced". As mentioned before, there are cons to deduplication. There is the problem of corrupted blocks, and a significant amount of overhead which leads to decreased performance. Because of these cons, there would be a need for even more backups when using deduplication as to ensure that you can still have your data, which can totally negate the effects of using this process. 

